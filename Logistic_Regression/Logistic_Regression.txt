INTRO TO LOGISTIC REGRESSION
1. What is  logistic regression?
2. What kind of problems solved?
3. Which situations we should use Logistic regression?

1. What is  logistic regression?
	Logistic regression is a statistical and machine learning technique for classifying records of a dataset, based on the values of the input fields.
	Letâ€™s say we have a telecommunication dataset that weâ€™d would like to analyze, in order	 to understand which customers might leave us next month.	 This is historical customer data where each row represents one customer.
	Youâ€™ll use the dataset to build a model based on historical records and use it to predict the future churn within the customer group.
	INDEPENDENT VARIABLE : 
		In logistic regression, independent variables should be continuous; if categorical, they should be dummy or indicator-coded. This means we have to transform them to some continuous value.
	DEPENDENT VARIABLE : 
		logistic regression can be used for both binary classification and multiclass classification

2. What kind of problems solved?
	a. To predict the "probability" of a person having a heart attack within a specified time period, based on our knowledge of the person's age, sex, and body mass index.
	b. To predict the chance of mortality in an injured patient, or to "predict" whether a patient has a given disease, such as diabetes, based on observed characteristics of that patient, such as weight, height, blood pressure, and results of various blood tests, and so on.
	c. In a marketing context, we can use it to predict the "likelihood" of a customer purchasing a product or halting a subscription, as weâ€™ve done in our churn example.
	d. To predict the "probability" of failure of a given process, system, or product.
	e. To predict the "likelihood" of a homeowner defaulting on a mortgage.

	\Notice that in all of these examples, not only do we predict the class of each case, we also measure the probability of a case belonging to a specific class.

3. Which situations we should use Logistic regression?
	Four situations in which Logistic regression is a good candidate:
		a. When the "target field in your data is categorical", or specifically, is binary, such as 0/1, yes/no, churn or no churn, positive/negative, and so on.
		
		b. When you "need the probability" of your prediction, for example, if you want to know what the probability is, of a customer buying a product. Logistic regression returns a probability score between 0 and 1 for a given sample of data. In fact, logistic regressing predicts the probability of that sample, and we map the cases to a discrete class based on that probability.
		
		c. If your data is "linearly separable".
				The decision boundary of logistic regression is a line or a plane or a hyper-plane. A classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. For example, if we have just two features (and are not applying any polynomial processing), we can obtain an inequality like Î¸_0+ Î¸_1 x_1+ Î¸_2 x_2 > 0, which is a half-plane, easily plottable.
				Please note that in using logistic regression, we can also achieve a complex decision boundary using polynomial processing as well, which is out of scope here.

		d. When you need to "understand the IMPACT" of a feature.
				You can select the best features based on the statistical significance of the logistic regression model coefficients or parameters.
				That is, after finding the optimum parameters, a feature x with the weight Î¸_1 close to 0, has a smaller effect on the prediction, than features with large absolute values of Î¸_1. Indeed, it allows us to understand the impact an independent variable has on the dependent variable while controlling other independent variables.

					Å·    = 		P(y = 1|X)
				P(y=0|X) = 1 -  P(y = 1|X)



LOGISTIC VS LINEAR REGRESSION
Steps:
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/LogR_steps.PNG"

Choosing Î¸ :
	Gradient Descent

Stopping Iteration :
	Satisfactory Accuracy Score.


LAB: LOGISTIC REGRESSION
While Linear Regression is suited for estimating continuous values (e.g. estimating house price), it is not the best tool for predicting the class of an observed data point. In order to estimate the class of a data point, we need some sort of guidance on what would be the most probable class for that data point. For this, we use Logistic Regression.

The objective of Logistic Regression algorithm, is to find the best parameters Î¸, for â„Ž_Î¸(ð‘¥) = ðœŽ({Î¸^TX}), in such a way that the model best predicts the class of each case.



# write your code here
parameters = {'C':[0.01,0.1,1,10],'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}
from sklearn.model_selection import GridSearchCV
#LR_1 = LogisticRegression(C=0.1, solver='liblinear').fit(X_train,y_train)
LR_1 = LogisticRegression()
cv = GridSearchCV(LR_1,parameters, cv = 5)
cv.fit(X_train,y_train)
print_results(cv)

print(log_loss(y_test, cv.best_estimator_.predict_proba(X_test)))
print (classification_report(y_test, cv.best_estimator_.predict(X_test)))
>>>

BEST PARAMS: {'C': 0.1, 'solver': 'liblinear'}

LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False)
                                params  mean_test_score
7    {'C': 0.1, 'solver': 'liblinear'}          0.76875
2   {'C': 0.01, 'solver': 'liblinear'}          0.76250
10     {'C': 1, 'solver': 'newton-cg'}          0.75000
14          {'C': 1, 'solver': 'saga'}          0.75000
5    {'C': 0.1, 'solver': 'newton-cg'}          0.75000


0.731 (+/-0.018) for {'C': 0.01, 'solver': 'newton-cg'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'lbfgs'}
0.762 (+/-0.191) for {'C': 0.01, 'solver': 'liblinear'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'sag'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'saga'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'newton-cg'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'lbfgs'}
0.769 (+/-0.156) for {'C': 0.1, 'solver': 'liblinear'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'sag'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'saga'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'newton-cg'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'lbfgs'}
0.744 (+/-0.145) for {'C': 1, 'solver': 'liblinear'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'sag'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'saga'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'newton-cg'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'lbfgs'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'liblinear'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'sag'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'saga'}
0.5739445725558303
              precision    recall  f1-score   support

           0       0.69      1.00      0.82        25
           1       1.00      0.27      0.42        15

   micro avg       0.72      0.72      0.73        40
   macro avg       0.85      0.63      0.62        40
weighted avg       0.81      0.72      0.67        40
